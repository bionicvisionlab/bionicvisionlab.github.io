<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VR/AR/MR on Bionic Vision Lab</title>
    <link>https://bionicvisionlab.org/tags/vr/ar/mr/</link>
    <description>Recent content in VR/AR/MR on Bionic Vision Lab</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year}</copyright>
    
	    <atom:link href="https://bionicvisionlab.org/tags/vr/ar/mr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>BionicVisionXR: An Open-Source Virtual Reality Toolbox for Bionic Vision</title>
      <link>https://bionicvisionlab.org/code/bionicvisionxr/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/code/bionicvisionxr/</guid>
      <description>&lt;p&gt;A major outstanding challenge in the field of bionic vision is predicting what people “see” when they use their devices.
The limited field of view of current devices necessitates head movements to scan the scene, which is difficult to simulate on a computer screen. 
In addition, many computational models of bionic vision lack biological realism.&lt;/p&gt;
&lt;p&gt;To address these challenges, we present &lt;em&gt;BionicVisionXR&lt;/em&gt;, an open-source virtual reality toolbox for simulated prosthetic vision that uses a psychophysically validated computational model to allow sighted participants to “see through the eyes” of a bionic
eye user.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards a Smart Bionic Eye</title>
      <link>https://bionicvisionlab.org/research/smart-bionic-eye/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/smart-bionic-eye/</guid>
      <description>&lt;p&gt;Rather than aiming to one day restore &lt;em&gt;natural&lt;/em&gt; vision (which may remain elusive until we fully understand the neural code of vision), we might be better off thinking about how to create &lt;em&gt;practical&lt;/em&gt; and &lt;em&gt;useful&lt;/em&gt; artificial vision now.
Specifically, a visual prosthesis has the potential to provide visual augmentations through the means of artificial intelligence (AI) based scene understanding (e.g., by highlighting important objects), tailored to specific real-world tasks that are known to affect the quality of life of people who are blind (e.g., face recognition, outdoor navigation, self-care).&lt;/p&gt;
&lt;p&gt;In the future, these visual augmentations could be combined with GPS to give directions, warn users of impending dangers in their immediate surroundings, or even extend the range of visible light with the use of an infrared sensor (think bionic night-time vision).
Once the quality of the generated artificial vision reaches a certain threshold, there are a lot of exciting avenues to pursue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Immersive Virtual Reality Simulations of Bionic Vision</title>
      <link>https://bionicvisionlab.org/research/immersive-virtual-reality-simulations/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/immersive-virtual-reality-simulations/</guid>
      <description>&lt;p&gt;Due to the unique requirements of working with bionic eye recipients (e.g., required assistance, increased setup time, travel cost), experimentation with different encoding methods remains challenging and expensive.&lt;/p&gt;
&lt;p&gt;Instead, embedding simulated prosthetic vision (SPV) models in immersive virtual reality (VR) allows sighted subjects to act as virtual patients by &amp;ldquo;seeing&amp;rdquo; through the eyes of the patient, taking into account their head and eye movements as they explore an immersive virtual environment.&lt;/p&gt;
&lt;p&gt;This can speed up the development process by allowing us to test theoretical predictions in high-throughput experiments, the best of which can be validated and improved upon in an iterative process with the bionic eye recipient in the loop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Navigation Under High-Stress Conditions</title>
      <link>https://bionicvisionlab.org/research/hight-stress-navigation/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/hight-stress-navigation/</guid>
      <description>&lt;p&gt;There are known individual differences in both ability to learn the layout of novel environments and flexibility of strategies for navigating known environments.
It is unclear, however, how navigational abilities and situational awareness are impacted by high-stress scenarios and whether augmented reality (AR) could be employed to enhance performance and situational awareness.&lt;/p&gt;
&lt;p&gt;This project will investigate three core questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does a person&amp;rsquo;s navigational abilities change in extreme situations?&lt;/li&gt;
&lt;li&gt;How can we best train them for these situations?&lt;/li&gt;
&lt;li&gt;How can vision augmentation be employed to improve situational awareness?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
