<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VR/AR/MR on Bionic Vision Lab</title>
    <link>https://bionicvisionlab.org/tags/vr/ar/mr/</link>
    <description>Recent content in VR/AR/MR on Bionic Vision Lab</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year}</copyright>
    
	    <atom:link href="https://bionicvisionlab.org/tags/vr/ar/mr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SimpleXR: An open-source Unity toolbox for simplified XR development</title>
      <link>https://bionicvisionlab.org/code/simplexr/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/code/simplexr/</guid>
      <description>&lt;p&gt;Extended reality (XR) is a powerful tool for human behavioral research. The ability to create 3D visual scenes and measure responses to arbitrary visual stimuli enables the behavioral researcher to test hypotheses in a well-controlled environment. However, software packages such as SteamVR, OpenXR, and ARKit have been developed for game designers rather than behavioral researchers. While Unity is considered the most beginner-friendly platform, barriers still exist for inexperienced programmers. Toolboxes such as VREX and USE have focused on simplifying experimental design and remote data collection, but no tools currently exist to help with all aspects of an experiment.&lt;/p&gt;
&lt;p&gt;To address this challenge, we have developed SimpleXR (sXR), an open-source Unity package that allows for creating complex experiments with relatively little code. The toolbox contains a plethora of tools that are particularly useful for the visual sciences, such as creating dynamic scenes, randomizing object locations, accessing eye-tracker data, and applying full-screen shader effects (e.g., blurring, gaze-contingent scotomas, edge detection) either in virtual reality (VR) or to the pass-through camera for augmented reality (AR) tasks. sXR also provides one-line commands for interacting with virtual objects, displaying stimuli and instructions, using timers, and much more. Additionally, it automatically switches between desktop and immersive VR modes. sXR creates separate user interfaces for the experimenter and participant, allowing the experimenter to track performance and monitor for anomalies. By using Unity’s Universal Rendering Pipeline, sXR allows researchers to develop across platforms, including VR headsets, AR glasses, and smartphones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BionicVisionXR: An Open-Source Virtual Reality Toolbox for Bionic Vision</title>
      <link>https://bionicvisionlab.org/code/bionicvisionxr/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/code/bionicvisionxr/</guid>
      <description>&lt;p&gt;A major outstanding challenge in the field of bionic vision is predicting what people “see” when they use their devices.
The limited field of view of current devices necessitates head movements to scan the scene, which is difficult to simulate on a computer screen. 
In addition, many computational models of bionic vision lack biological realism.&lt;/p&gt;
&lt;p&gt;To address these challenges, we present &lt;em&gt;BionicVisionXR&lt;/em&gt;, an open-source virtual reality toolbox for simulated prosthetic vision that uses a psychophysically validated computational model to allow sighted participants to “see through the eyes” of a bionic
eye user.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards a Smart Bionic Eye</title>
      <link>https://bionicvisionlab.org/research/smart-bionic-eye/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/smart-bionic-eye/</guid>
      <description>&lt;p&gt;Rather than aiming to one day restore &lt;em&gt;natural&lt;/em&gt; vision (which may remain elusive until we fully understand the neural code of vision), we might be better off thinking about how to create &lt;em&gt;practical&lt;/em&gt; and &lt;em&gt;useful&lt;/em&gt; artificial vision now.
Specifically, a visual prosthesis has the potential to provide visual augmentations through the means of artificial intelligence (AI) based scene understanding (e.g., by highlighting important objects), tailored to specific real-world tasks that are known to affect the quality of life of people who are blind (e.g., face recognition, outdoor navigation, self-care).&lt;/p&gt;
&lt;p&gt;In the future, these visual augmentations could be combined with GPS to give directions, warn users of impending dangers in their immediate surroundings, or even extend the range of visible light with the use of an infrared sensor (think bionic night-time vision).
Once the quality of the generated artificial vision reaches a certain threshold, there are a lot of exciting avenues to pursue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assistive Technologies for People Who Are Blind</title>
      <link>https://bionicvisionlab.org/research/assistive-technologies-blind/</link>
      <pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/assistive-technologies-blind/</guid>
      <description>&lt;p&gt;Globally, millions of individuals with visual impairments face significant challenges in navigation and independence. Traditional white canes, while helpful, offer limited assistance in complex environments. Accessible tech, once effective, often turns commercial with hefty prices. This hits harder due to the high unemployment rate among the visually impaired.&lt;/p&gt;
&lt;p&gt;Our goal is to leverage computer vision and AI to co-design the next-generation of accessible technologies with people who are blind, focusing on affordable design and open-source software, thereby enabling community-driven development and widespread impact.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Navigation Under High-Stress Conditions</title>
      <link>https://bionicvisionlab.org/research/high-stress-navigation/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/high-stress-navigation/</guid>
      <description>&lt;p&gt;There are known individual differences in both ability to learn the layout of novel environments and flexibility of strategies for navigating known environments.
It is unclear, however, how navigational abilities and situational awareness are impacted by high-stress scenarios and whether augmented reality (AR) could be employed to enhance performance and situational awareness.&lt;/p&gt;
&lt;p&gt;This project will investigate three core questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does a person&amp;rsquo;s navigational abilities change in extreme situations?&lt;/li&gt;
&lt;li&gt;How can we best train them for these situations?&lt;/li&gt;
&lt;li&gt;How can vision augmentation be employed to improve situational awareness?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
