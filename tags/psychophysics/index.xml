<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Psychophysics on Bionic Vision Lab</title>
    <link>https://bionicvisionlab.org/tags/psychophysics/</link>
    <description>Recent content in Psychophysics on Bionic Vision Lab</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year}</copyright>
    <lastBuildDate>Wed, 01 Dec 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://bionicvisionlab.org/tags/psychophysics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>End-to-End Optimization of Bionic Vision</title>
      <link>https://bionicvisionlab.org/research/end-to-end-optimization/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/end-to-end-optimization/</guid>
      <description>&lt;p&gt;Our lack of understanding of multi-electrode interactions severely limits current stimulation protocols. For example, current Argus II protocols simply attempt to minimize electric field interactions by maximizing phase delays across electrodes using ‘time-multiplexing’. The assumption is that single-electrode percepts act as atomic ‘building blocks’ of patterned vision. However, these building blocks often fail to assemble into more complex percepts.&lt;/p&gt;
&lt;p&gt;The goal of this project is therefore to develop new stimulation strategies that minimize perceptual distortions.
One potential avenue is to view this as an end-to-end optimization problem, where a deep neural network (encoder) is trained to predict the electrical stimulus needed to produce a desired percept (target).&lt;/p&gt;
&lt;p&gt;Importantly, this model would have to be trained with the phosphene model in the loop, such that the overall network would minimize a perceptual error between the predicted and target output.
This is technically challenging, because a phosphene model must be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;simple enough to be differentiable such that it can be included in the backward pass of a deep neural network,&lt;/li&gt;
&lt;li&gt;complex enough to be able to explain the spatiotemporal perceptual distortions observed in real prosthesis patients, and&lt;/li&gt;
&lt;li&gt;amenable to an efficient implementation such that the training of the network is feasible.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Visual Outcomes for Visual Prostheses</title>
      <link>https://bionicvisionlab.org/research/predicting-visual-outcomes-visual-prostheses/</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/predicting-visual-outcomes-visual-prostheses/</guid>
      <description>&lt;p&gt;A major outstanding challenge is predicting what people &amp;ldquo;see&amp;rdquo; when they use their devices.&lt;/p&gt;
&lt;p&gt;Instead of seeing focal spots of light, current visual implant users perceive highly distorted percepts, which vary in shape not just across subjects but also across electrodes and often fail to assemble into more complex percepts.
Furthermore, phosphenes appear fundamentally different depending on whether they are generated with retinal or cortical implants.&lt;/p&gt;
&lt;p&gt;The goal of this project is thus to combine psychophysical and neuroanatomical data that can inform phosphene models capable of linking electrical stimulation directly to perception.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Information Needs of People Who Are Blind or Visually Impaired</title>
      <link>https://bionicvisionlab.org/research/information-needs-blind-low-vision/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/information-needs-blind-low-vision/</guid>
      <description>&lt;p&gt;The goal of this project is to obtain a nuanced understanding of the strategies that people who are blind or visually impaired (BVI) employ to perform different instrumental activities of daily living (iADLs).&lt;/p&gt;
&lt;p&gt;Identifying useful and relevant visual cues that could support these iADLs, especially when the task involves some level of scene understanding, orientation, and mobility, will be essential to the success of near-future visual accessibility aids.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Visual Impairment</title>
      <link>https://bionicvisionlab.org/research/simulated-visual-impairment/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/simulated-visual-impairment/</guid>
      <description>&lt;p&gt;How are visual acuity and daily activities affected by visual impairment?&lt;/p&gt;
&lt;p&gt;Previous studies with people who have retinal degeneration have shown that vision is altered and impaired in the presence of a scotoma. This is also the case when a sighted person is tested under simulated low vision (SLV) conditions. However, the extent to which patient-specific factors affect vision and quality of life is not well understood.&lt;/p&gt;
&lt;p&gt;Testing sighted participants with SLV allows us to compare performance to real patients, design simulations to be as naturalistic as possible, and assess changes in vision for real life tasks instead of relying on acuity alone.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pulse2percept: A Python-Based Simulation Framework for Bionic Vision</title>
      <link>https://bionicvisionlab.org/code/pulse2percept/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/code/pulse2percept/</guid>
      <description>&lt;p&gt;&lt;em&gt;pulse2percept&lt;/em&gt; is a BSD-licensed, open-source Python package for simulated prosthetic vision (SPV).&lt;/p&gt;
&lt;p&gt;Built on the NumPy and SciPy stacks, as well as contributions from the broader Python community, &lt;em&gt;pulse2percept&lt;/em&gt; provides an open-source implementation of several phosphene models for a wide range of state-of-the-art retinal prostheses, to provide insight into the visual experience provided by these devices.&lt;/p&gt;
&lt;p&gt;The project started at the University of Washington under the guidance of &lt;a href=&#34;https://psych.uw.edu/people/2400/ione-fine&#34; target=&#34;_blank&#34;&gt;Ione Fine&lt;/a&gt;, &lt;a href=&#34;https://psych.uw.edu/people/3572/geoffrey-boynton&#34; target=&#34;_blank&#34;&gt;Geoff Boynton&lt;/a&gt;, and &lt;a href=&#34;https://psych.uw.edu/people/8823/ariel-rokem&#34; target=&#34;_blank&#34;&gt;Ariel Rokem&lt;/a&gt;.
It has since been repackaged to run on both CPU and GPU backends, extended to support both retinal and cortical prostheses, and upgraded to be compatible with the Open Neural Network Exchange (ONNX) standard.&lt;/p&gt;
&lt;p&gt;As &lt;em&gt;pulse2percept&lt;/em&gt; continues to be adopted by several research labs around the globe, we continue to improve its functionality and performance as well as add new implants, models, and datasets.&lt;/p&gt;
&lt;p&gt;Documentation is available at &lt;a href=&#34;https://pulse2percept.readthedocs.io&#34;&gt;https://pulse2percept.readthedocs.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contribute at &lt;a href=&#34;https://github.com/pulse2percept/pulse2percept&#34;&gt;https://github.com/pulse2percept/pulse2percept&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width=&#34;760&#34; height=&#34;427&#34; src=&#34;https://www.youtube.com/embed/KxsNAa-P2X4&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Visual Navigation Under High-Stress Conditions</title>
      <link>https://bionicvisionlab.org/research/high-stress-navigation/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bionicvisionlab.org/research/high-stress-navigation/</guid>
      <description>&lt;p&gt;There are known individual differences in both ability to learn the layout of novel environments and flexibility of strategies for navigating known environments.
It is unclear, however, how navigational abilities and situational awareness are impacted by high-stress scenarios and whether augmented reality (AR) could be employed to enhance performance and situational awareness.&lt;/p&gt;
&lt;p&gt;This project will investigate three core questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does a person&amp;rsquo;s navigational abilities change in extreme situations?&lt;/li&gt;
&lt;li&gt;How can we best train them for these situations?&lt;/li&gt;
&lt;li&gt;How can vision augmentation be employed to improve situational awareness?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
